{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project scope\n",
    "Model design and training for Singaporean nationality, gender and race prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing\n",
    "install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observe data using VScode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Nationality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ho Siew Lai</td>\n",
       "      <td>Female</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singaporean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ng Beng Nam</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singaporean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Esther Tan</td>\n",
       "      <td>Female</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singaporean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pg Gwee</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singaporean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nayli Qistina</td>\n",
       "      <td>Female</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singaporean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Gender     Race  Nationality\n",
       "0    Ho Siew Lai  Female  Chinese  Singaporean\n",
       "1    Ng Beng Nam    Male  Chinese  Singaporean\n",
       "2     Esther Tan  Female  Chinese  Singaporean\n",
       "3        Pg Gwee    Male  Chinese  Singaporean\n",
       "4  Nayli Qistina  Female  Chinese  Singaporean"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "raw_data=pd.read_csv('../data/training.csv')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data cleaning\n",
    "remove missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>Nationality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ho Siew Lai</td>\n",
       "      <td>Female</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singaporean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ng Beng Nam</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singaporean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Esther Tan</td>\n",
       "      <td>Female</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singaporean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pg Gwee</td>\n",
       "      <td>Male</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singaporean</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nayli Qistina</td>\n",
       "      <td>Female</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>Singaporean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Gender     Race  Nationality\n",
       "0    Ho Siew Lai  Female  Chinese  Singaporean\n",
       "1    Ng Beng Nam    Male  Chinese  Singaporean\n",
       "2     Esther Tan  Female  Chinese  Singaporean\n",
       "3        Pg Gwee    Male  Chinese  Singaporean\n",
       "4  Nayli Qistina  Female  Chinese  Singaporean"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_data(raw_data):\n",
    "    # Drop rows with missing data across all columns\n",
    "    raw_data = raw_data.dropna()\n",
    "    # Drop duplicate rows across all columns\n",
    "    raw_data = raw_data.drop_duplicates()\n",
    "    # Filter rows based on columns: 'Gender', 'Race', 'Nationality'\n",
    "    raw_data = raw_data[(raw_data['Gender'] != \"Unknown\") & (raw_data['Race'] != \"Unknown\") & (raw_data['Nationality'] != \"Unknown\")]\n",
    "    # Filter rows based on columns: 'Gender', 'Race', 'Nationality'\n",
    "    raw_data = raw_data[(raw_data['Gender'] != \"Not Specified\") & (raw_data['Race'] != \"Not Specified\") & (raw_data['Nationality'] != \"Not Specified\")]\n",
    "    # Filter rows based on columns: 'Race', 'Nationality'\n",
    "    raw_data = raw_data[(raw_data['Race'] != \"Not specified\") & (raw_data['Nationality'] != \"Not specified\")]\n",
    "    # Replace 'Unknown (Non-specific)' with 'Non-specific'\n",
    "    raw_data['Race'] = raw_data['Race'].replace('Unknown (Non-specific)', 'Non-specific')\n",
    "    return raw_data\n",
    "\n",
    "raw_data_clean = clean_data(raw_data.copy())\n",
    "raw_data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8862, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deletes titles in name and remove leading/trailing space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8862, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regular expression pattern for common titles (only at the beginning of the string)\n",
    "pattern = r'^\\b(Mr\\.?|Mrs\\.?|Ms\\.?|Dr\\.?|Doc\\.?|Prof\\.?|Sir|Madam|Miss)\\b'\n",
    "\n",
    "# Remove the titles from the \"name\" column\n",
    "raw_data_clean['Name'] = raw_data_clean['Name'].replace(pattern, '', regex=True)\n",
    "\n",
    "# Strip leading/trailing whitespace\n",
    "raw_data_clean['Name'] = raw_data_clean['Name'].str.strip()\n",
    "raw_data_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame to a CSV file\n",
    "raw_data_clean.to_csv('../data/data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Text data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Gender_Male</th>\n",
       "      <th>Race_African</th>\n",
       "      <th>Race_Arabic</th>\n",
       "      <th>Race_Bangladeshi</th>\n",
       "      <th>Race_Bengali</th>\n",
       "      <th>Race_British</th>\n",
       "      <th>Race_Burmese</th>\n",
       "      <th>Race_Caucasian</th>\n",
       "      <th>Race_Chinese</th>\n",
       "      <th>...</th>\n",
       "      <th>Nationality_Nepali</th>\n",
       "      <th>Nationality_Other</th>\n",
       "      <th>Nationality_Russian</th>\n",
       "      <th>Nationality_Singapore</th>\n",
       "      <th>Nationality_Singaporean</th>\n",
       "      <th>Nationality_Spanish</th>\n",
       "      <th>Nationality_Sri Lankan</th>\n",
       "      <th>Nationality_Thai</th>\n",
       "      <th>Nationality_Turkish</th>\n",
       "      <th>Nationality_Vietnamese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ho Siew Lai</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ng Beng Nam</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Esther Tan</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pg Gwee</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nayli Qistina</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Name  Gender_Male  Race_African  Race_Arabic  Race_Bangladeshi  \\\n",
       "0    Ho Siew Lai            0         False        False             False   \n",
       "1    Ng Beng Nam            1         False        False             False   \n",
       "2     Esther Tan            0         False        False             False   \n",
       "3        Pg Gwee            1         False        False             False   \n",
       "4  Nayli Qistina            0         False        False             False   \n",
       "\n",
       "   Race_Bengali  Race_British  Race_Burmese  Race_Caucasian  Race_Chinese  \\\n",
       "0         False         False         False           False          True   \n",
       "1         False         False         False           False          True   \n",
       "2         False         False         False           False          True   \n",
       "3         False         False         False           False          True   \n",
       "4         False         False         False           False          True   \n",
       "\n",
       "   ...  Nationality_Nepali  Nationality_Other  Nationality_Russian  \\\n",
       "0  ...               False              False                False   \n",
       "1  ...               False              False                False   \n",
       "2  ...               False              False                False   \n",
       "3  ...               False              False                False   \n",
       "4  ...               False              False                False   \n",
       "\n",
       "   Nationality_Singapore  Nationality_Singaporean  Nationality_Spanish  \\\n",
       "0                  False                     True                False   \n",
       "1                  False                     True                False   \n",
       "2                  False                     True                False   \n",
       "3                  False                     True                False   \n",
       "4                  False                     True                False   \n",
       "\n",
       "   Nationality_Sri Lankan  Nationality_Thai  Nationality_Turkish  \\\n",
       "0                   False             False                False   \n",
       "1                   False             False                False   \n",
       "2                   False             False                False   \n",
       "3                   False             False                False   \n",
       "4                   False             False                False   \n",
       "\n",
       "   Nationality_Vietnamese  \n",
       "0                   False  \n",
       "1                   False  \n",
       "2                   False  \n",
       "3                   False  \n",
       "4                   False  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_data(raw_data_clean):\n",
    "    # Multi-label encode column 'Gender' using delimiter 'Female'\n",
    "    loc_0 = raw_data_clean.columns.get_loc('Gender')\n",
    "    raw_data_clean_encoded = raw_data_clean['Gender'].str.get_dummies(sep='Female').add_prefix('Gender_')\n",
    "    raw_data_clean = pd.concat([raw_data_clean.iloc[:,:loc_0], raw_data_clean_encoded, raw_data_clean.iloc[:,loc_0+1:]], axis=1)\n",
    "    # One-hot encode column: 'Race'\n",
    "    raw_data_clean = pd.get_dummies(raw_data_clean, columns=['Race'])\n",
    "    # One-hot encode column: 'Nationality'\n",
    "    raw_data_clean = pd.get_dummies(raw_data_clean, columns=['Nationality'])\n",
    "    return raw_data_clean\n",
    "\n",
    "data_encoded = clean_data(raw_data_clean.copy())\n",
    "data_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_race(data_encoded):\n",
    "     # Define the race columns to keep\n",
    "    races_to_keep = ['Race_Chinese', 'Race_Indian', 'Race_Malay']\n",
    "\n",
    "    # Identify all race columns\n",
    "    race_columns = [col for col in data_encoded.columns if col.startswith('Race_')]\n",
    "\n",
    "    # Columns to be summed into 'Race_Other'\n",
    "    other_race_columns = [col for col in race_columns if col not in races_to_keep]\n",
    "\n",
    "    # Create 'Race_Other' column by summing the values of the other race columns\n",
    "    data_encoded['Race_Other'] = data_encoded[other_race_columns].sum(axis=1) > 0\n",
    "\n",
    "    # Ensure 'Race_Other' is not in the list of columns to drop\n",
    "    other_race_columns = [col for col in other_race_columns if col != 'Race_Other']\n",
    "    \n",
    "    # Drop the original other race columns\n",
    "    data_encoded.drop(columns=other_race_columns, inplace=True)\n",
    "\n",
    "    return data_encoded\n",
    "\n",
    "# Apply the function to your encoded data\n",
    "data_encoded = merge_race(data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nationality(data_encoded):\n",
    "     # Define the race columns to keep\n",
    "    nationality_to_keep = ['Nationality_Singaporean', 'Nationality_Singapore']\n",
    "\n",
    "    # Identify all race columns\n",
    "    nationality_columns = [col for col in data_encoded.columns if col.startswith('Nationality_')]\n",
    "\n",
    "    # Columns to be summed into 'Race_Other'\n",
    "    other_nationality_columns = [col for col in nationality_columns if col not in nationality_to_keep]\n",
    "\n",
    "    # Create 'Race_Other' column by summing the values of the other race columns\n",
    "    data_encoded['Nationality_Foreigner'] = data_encoded[other_nationality_columns].sum(axis=1) > 0\n",
    "\n",
    "    # Ensure 'Race_Other' is not in the list of columns to drop\n",
    "    other_nationality_columns = [col for col in other_nationality_columns if col != 'Nationality_Foreigner']\n",
    "    \n",
    "    # Drop the original other race columns\n",
    "    data_encoded.drop(columns=other_nationality_columns, inplace=True)\n",
    "\n",
    "    data_encoded['Nationality_Singaporean']= data_encoded['Nationality_Singaporean']+ data_encoded['Nationality_Singapore']\n",
    "    data_encoded.drop(columns=['Nationality_Singapore'], inplace=True)\n",
    "    return data_encoded\n",
    "\n",
    "# Apply the function to your encoded data\n",
    "data_encoded = merge_nationality(data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "export backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export DataFrame to a CSV file\n",
    "data_encoded.to_csv('../data/data_encoded.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model structure design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 09:00:55.312948: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-05 09:00:55.334432: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-05 09:00:55.334456: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-05 09:00:55.335004: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-05 09:00:55.338947: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_base_en_uncased\n",
    "# 12-layer BERT model where all input is lowercased. Trained on English Wikipedia + BooksCorpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras_nlp.models import BertTokenizer\n",
    "\n",
    "# tokenizer = BertTokenizer.from_preset(\"bert_base_en\")\n",
    "# tokenized_names = [tokenizer(name) for name in data_encoded['Name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 09:00:58.686509: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-05 09:00:58.689782: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-05 09:00:58.689810: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-05 09:00:58.693177: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-05 09:00:58.693204: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-05 09:00:58.693216: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-05 09:00:59.440624: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-05 09:00:59.440663: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-05 09:00:59.440669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-01-05 09:00:59.440688: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-01-05 09:00:59.440707: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13338 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp\n",
    "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_base_en\")\n",
    "import tensorflow as tf\n",
    "names = tf.constant(data_encoded['Name'].to_list())\n",
    "preprocessed_data = preprocessor(names)\n",
    "token_ids = preprocessed_data['token_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Determine the maximum length from the token IDs\n",
    "max_length = token_ids.shape[1]\n",
    "# segment_ids are all zeros, as there's only one segment (single sentence/names)\n",
    "segment_ids = np.zeros_like(token_ids)\n",
    "\n",
    "# padding_mask is 1 where token_ids are not zero (actual tokens), and 0 where they are zero (padding)\n",
    "padding_mask = np.where(token_ids != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of race-related columns\n",
    "num_race_classes = len([col for col in data_encoded.columns if col.startswith('Race_')])\n",
    "\n",
    "# Counting the number of nationality-related columns\n",
    "num_nationality_classes = len([col for col in data_encoded.columns if col.startswith('Nationality_')])\n",
    "\n",
    "gender_labels = data_encoded['Gender_Male'].values\n",
    "gender_labels = gender_labels.reshape(-1, 1)\n",
    "\n",
    "race_columns = [col for col in data_encoded.columns if col.startswith('Race_')]\n",
    "race_labels = data_encoded[race_columns].values\n",
    "\n",
    "nationality_columns = [col for col in data_encoded.columns if col.startswith('Nationality_')]\n",
    "nationality_labels = data_encoded[nationality_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stack the labels along the second axis (axis=1)\n",
    "labels = np.concatenate([gender_labels, race_labels, nationality_labels], axis=1)\n",
    "\n",
    "# Convert the token_ids (Tensor) to a NumPy array\n",
    "token_ids_np = token_ids.numpy()\n",
    "\n",
    "inputs = np.concatenate([token_ids_np, segment_ids, padding_mask], axis=-1)\n",
    "# Now use train_test_split with the NumPy array\n",
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
    "    inputs, \n",
    "    labels,\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming each component has the same length\n",
    "length = max_length  # replace with actual length of each component\n",
    "\n",
    "train_token_ids = train_inputs[:, :length]\n",
    "train_segment_ids = train_inputs[:, length:2*length]\n",
    "train_padding_mask = train_inputs[:, 2*length:]\n",
    "\n",
    "test_token_ids = test_inputs[:, :length]\n",
    "test_segment_ids = test_inputs[:, length:2*length]\n",
    "test_padding_mask = test_inputs[:, 2*length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the first column is gender, the next 'num_race_classes' columns are for race, and the rest are for nationality\n",
    "train_gender_labels = train_labels[:, 0]\n",
    "train_race_labels = train_labels[:, 1:1+num_race_classes]\n",
    "train_nationality_labels = train_labels[:, 1+num_race_classes:]\n",
    "\n",
    "test_gender_labels = test_labels[:, 0]\n",
    "test_race_labels = test_labels[:, 1:1+num_race_classes]\n",
    "test_nationality_labels = test_labels[:, 1+num_race_classes:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_labels(labels, smoothing_factor=0.1):\n",
    "    num_classes = labels.shape[1]  # Assuming one-hot encoded labels\n",
    "    smooth_value = smoothing_factor / num_classes\n",
    "    new_labels = labels * (1 - smoothing_factor) + smooth_value\n",
    "    return new_labels\n",
    "\n",
    "# Apply label smoothing\n",
    "alpha = 0.1  # Example smoothing factor\n",
    "\n",
    "# Apply smoothing to race and nationality labels only (as gender labels are binary)\n",
    "train_race_labels = smooth_labels(train_race_labels, alpha)\n",
    "train_nationality_labels = smooth_labels(train_nationality_labels, alpha)\n",
    "\n",
    "test_race_labels = smooth_labels(test_race_labels, alpha)\n",
    "test_nationality_labels = smooth_labels(test_nationality_labels, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_gender_labels, train_race_labels, train_nationality_labels are your training labels\n",
    "\n",
    "# Compute class weights for Gender\n",
    "weights_gender = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(train_gender_labels),\n",
    "    y=train_gender_labels\n",
    ")\n",
    "weights_gender = {i: weight for i, weight in enumerate(weights_gender)}\n",
    "\n",
    "# Compute class weights for Race\n",
    "weights_race = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(np.argmax(train_race_labels, axis=1)),\n",
    "    y=np.argmax(train_race_labels, axis=1)\n",
    ")\n",
    "weights_race = {i: weight for i, weight in enumerate(weights_race)}\n",
    "\n",
    "# Compute class weights for Nationality\n",
    "weights_nationality = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(np.argmax(train_nationality_labels, axis=1)),\n",
    "    y=np.argmax(train_nationality_labels, axis=1)\n",
    ")\n",
    "weights_nationality = {i: weight for i, weight in enumerate(weights_nationality)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample weights for gender\n",
    "sample_weights_gender = np.array([weights_gender[label] for label in train_gender_labels])\n",
    "\n",
    "# Create sample weights for race\n",
    "sample_weights_race = np.array([weights_race[label] for label in np.argmax(train_race_labels, axis=1)])\n",
    "\n",
    "# Create sample weights for nationality\n",
    "sample_weights_nationality = np.array([weights_nationality[label] for label in np.argmax(train_nationality_labels, axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sample_weights = (sample_weights_gender + sample_weights_race + sample_weights_nationality) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_nlp.models import BertBackbone\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import L1,L2\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.initializers import RandomUniform\n",
    "\n",
    "# Define the inputs\n",
    "input_ids = Input(shape=(max_length,), dtype='int32', name='input_ids')\n",
    "segment_ids = Input(shape=(max_length,), dtype='int32', name='segment_ids')\n",
    "padding_mask = Input(shape=(max_length,), dtype='int32', name='padding_mask')\n",
    "\n",
    "backbone = BertBackbone.from_preset(\"bert_base_en\")\n",
    "backbone_output = backbone({'token_ids': input_ids, 'segment_ids': segment_ids, 'padding_mask': padding_mask})\n",
    "# Assuming 'pooled_output' is the pooled output from BERT\n",
    "cls_output = backbone_output['pooled_output']\n",
    "\n",
    "\n",
    "# Add Dropout and L2 Regularization in the classification layers\n",
    "initializer = RandomUniform(minval=0.0, maxval=1.0)\n",
    "classifier_layer = Dense(128, activation='relu', \n",
    "                         kernel_regularizer=L1(0.01),\n",
    "                        activity_regularizer=L2(0.01),\n",
    "                        kernel_initializer=initializer)(cls_output)\n",
    "batch_norm = BatchNormalization()(classifier_layer)\n",
    "dropout = Dropout(0.3)(batch_norm)\n",
    "gender_output = Dense(1, \n",
    "                      activation='sigmoid', \n",
    "                      name='gender_output',\n",
    "                      kernel_regularizer=L1(0.01),\n",
    "                        activity_regularizer=L2(0.01),\n",
    "                        kernel_initializer=initializer)(dropout)\n",
    "# Define output layers for each classification task\n",
    "race_output = Dense(num_race_classes, activation='softmax', name='race_output',kernel_regularizer=L1(0.01),\n",
    "                        activity_regularizer=L2(0.01),\n",
    "                        kernel_initializer=initializer)(dropout)\n",
    "nationality_output = Dense(num_nationality_classes, activation='softmax', name='nationality_output',kernel_regularizer=L1(0.01),\n",
    "                        activity_regularizer=L2(0.01),\n",
    "                        kernel_initializer=initializer)(dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_ids,segment_ids,padding_mask], outputs=[gender_output, race_output, nationality_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "callback =EarlyStopping(monitor=\"val_loss\",\n",
    "    patience=0,\n",
    "    mode=\"auto\",\n",
    "    restore_best_weights=True,\n",
    "    start_from_epoch=5,\n",
    ")\n",
    "\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss={\n",
    "        'gender_output': 'binary_crossentropy',\n",
    "        'race_output': 'categorical_crossentropy',\n",
    "        'nationality_output': 'categorical_crossentropy'\n",
    "    },\n",
    "    metrics={\n",
    "        'gender_output': ['accuracy'],\n",
    "        'race_output': ['accuracy'],\n",
    "        'nationality_output': ['accuracy']\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-05 09:01:25.211290: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f080801e990 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-05 09:01:25.211320: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090 Laptop GPU, Compute Capability 8.9\n",
      "2024-01-05 09:01:25.704210: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-05 09:01:32.706758: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1704445332.582687     521 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "W0000 00:00:1704445332.690372     521 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m708/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - gender_output_accuracy: 0.6467 - loss: 7529.7876 - nationality_output_accuracy: 0.8670 - race_output_accuracy: 0.6881"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1704445470.284520     518 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 211ms/step - gender_output_accuracy: 0.6468 - loss: 7513.3574 - nationality_output_accuracy: 0.8670 - race_output_accuracy: 0.6881 - val_gender_output_accuracy: 0.6671 - val_loss: 413.0852 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 2/30\n",
      "\u001b[1m  1/709\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 143ms/step - gender_output_accuracy: 0.8750 - loss: 413.1082 - nationality_output_accuracy: 0.8750 - race_output_accuracy: 0.6250"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1704445482.578834     517 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 141ms/step - gender_output_accuracy: 0.6599 - loss: 398.3272 - nationality_output_accuracy: 0.8936 - race_output_accuracy: 0.7050 - val_gender_output_accuracy: 0.6671 - val_loss: 346.7770 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 3/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 141ms/step - gender_output_accuracy: 0.6718 - loss: 325.2985 - nationality_output_accuracy: 0.9010 - race_output_accuracy: 0.7213 - val_gender_output_accuracy: 0.6671 - val_loss: 255.1484 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 4/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 142ms/step - gender_output_accuracy: 0.6654 - loss: 228.7589 - nationality_output_accuracy: 0.8953 - race_output_accuracy: 0.7084 - val_gender_output_accuracy: 0.6671 - val_loss: 148.0195 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 5/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 142ms/step - gender_output_accuracy: 0.6680 - loss: 121.6521 - nationality_output_accuracy: 0.8978 - race_output_accuracy: 0.7203 - val_gender_output_accuracy: 0.6671 - val_loss: 49.9086 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 6/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 141ms/step - gender_output_accuracy: 0.6604 - loss: 32.9668 - nationality_output_accuracy: 0.8920 - race_output_accuracy: 0.7125 - val_gender_output_accuracy: 0.6671 - val_loss: 3.7700 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 7/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 142ms/step - gender_output_accuracy: 0.6653 - loss: 3.0699 - nationality_output_accuracy: 0.8933 - race_output_accuracy: 0.7070 - val_gender_output_accuracy: 0.6671 - val_loss: 2.9597 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 8/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 142ms/step - gender_output_accuracy: 0.6563 - loss: 2.7810 - nationality_output_accuracy: 0.8962 - race_output_accuracy: 0.7243 - val_gender_output_accuracy: 0.6671 - val_loss: 2.9335 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 9/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 142ms/step - gender_output_accuracy: 0.6559 - loss: 2.8471 - nationality_output_accuracy: 0.8967 - race_output_accuracy: 0.7063 - val_gender_output_accuracy: 0.6671 - val_loss: 2.9942 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 10/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 141ms/step - gender_output_accuracy: 0.6525 - loss: 2.9023 - nationality_output_accuracy: 0.8954 - race_output_accuracy: 0.7129 - val_gender_output_accuracy: 0.6671 - val_loss: 3.0891 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 11/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 142ms/step - gender_output_accuracy: 0.6544 - loss: 3.0026 - nationality_output_accuracy: 0.8966 - race_output_accuracy: 0.7056 - val_gender_output_accuracy: 0.6671 - val_loss: 3.1773 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 12/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 142ms/step - gender_output_accuracy: 0.6570 - loss: 3.0778 - nationality_output_accuracy: 0.9004 - race_output_accuracy: 0.7126 - val_gender_output_accuracy: 0.6671 - val_loss: 3.3066 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 13/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 141ms/step - gender_output_accuracy: 0.6517 - loss: 4.1974 - nationality_output_accuracy: 0.8873 - race_output_accuracy: 0.7066 - val_gender_output_accuracy: 0.6671 - val_loss: 3.5132 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 14/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 142ms/step - gender_output_accuracy: 0.6742 - loss: 39.3057 - nationality_output_accuracy: 0.8947 - race_output_accuracy: 0.7089 - val_gender_output_accuracy: 0.6671 - val_loss: 6.6881 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 15/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 142ms/step - gender_output_accuracy: 0.6666 - loss: 5.5561 - nationality_output_accuracy: 0.9008 - race_output_accuracy: 0.7187 - val_gender_output_accuracy: 0.6671 - val_loss: 4.4963 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 16/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 142ms/step - gender_output_accuracy: 0.6626 - loss: 4.1889 - nationality_output_accuracy: 0.8941 - race_output_accuracy: 0.7117 - val_gender_output_accuracy: 0.6671 - val_loss: 4.1967 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 17/30\n",
      "\u001b[1m709/709\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 141ms/step - gender_output_accuracy: 0.6644 - loss: 3.9597 - nationality_output_accuracy: 0.8984 - race_output_accuracy: 0.7129 - val_gender_output_accuracy: 0.6671 - val_loss: 4.0201 - val_nationality_output_accuracy: 0.8822 - val_race_output_accuracy: 0.6911\n",
      "Epoch 18/30\n",
      "\u001b[1m193/709\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 132ms/step - gender_output_accuracy: 0.6804 - loss: 3.8590 - nationality_output_accuracy: 0.8856 - race_output_accuracy: 0.6996"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    {'input_ids': train_token_ids, 'segment_ids': train_segment_ids, 'padding_mask': train_padding_mask},\n",
    "    {'gender_output': train_gender_labels, 'race_output': train_race_labels, 'nationality_output': train_nationality_labels},\n",
    "    # class_weight={\n",
    "    #     'gender_output': weights_gender,\n",
    "    #     'race_output': weights_race,\n",
    "    #     'nationality_output': weights_nationality\n",
    "    # },\n",
    "    sample_weight=combined_sample_weights,\n",
    "    epochs=30,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    verbose=\"auto\",\n",
    "    validation_split=0.2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
